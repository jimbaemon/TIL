# 쿠버네티스 핵심 개념

## 큐브 시스템 컴포넌트

### 1. 큐브 API 서버

* 쿠버네티스 시스템 컴포넌트는 오직 API 서버와 통신한다.
  * 컴포넌트끼리 서로 통신하는 일은 없다.
  * 컴포넌트끼리 서로 통신할때는 API 서버를 중계하여야 함
* RestfulAPI를 통해 클러스터 상태를 쿼리, 수정할 수 잇는 기능 제공
* API 서버의 구체적인 역활
  * 인증 플러그인을 사용한 클라이언트 인증
  * 권한 승인 플러그인을 통한 클라이언트 인증
  * 승인 제어 플러그인을 통해 요청 받은 리소스를 확인/수정
  * 리소스 검증 및 영구 저장



### 2. 큐브 컨트롤러 매니저

* 컨트롤러에는 다양한 컨트롤러가 존재
* API에 의해 받아진 요청을 처리하는 역활
  * 레플리케이션 매너저
  * 레플리카셋, 데몬셋, 잡 컨트롤러
  * 디플로이먼트 컨트롤러
  * ...



### 3. 큐브 스케줄러

* 일반적으로 실행할 노드를 직접 정해주지 않음
* 요청 받은 리소스를 어느 노드에 실행할지 경정하는 역활
* 현재 노드의 상태를 점검하고 최상의 노드를 찾아 배치
* 다수의 포드를 배치하는 경우에는 `라운드로빈`을 사용하여 분산



## 큐브시스템에서 사용하는 포드 확인

```bash
kubectl get pod -n kube-system
```

**실행결과**

![image-20210220212811518](http://www.jimbae.com:59005/image/154)



## 큐브시스템 설정 변경법

* `/etc/kubernetes/manifests/` 하위의 `.yml` 파일의 설정을 변경하면 된다.





## ETCD 데이터베이스 살펴보기

* etcd 는 쿠버네티스에서 사용 하는 Open Source 데이터베이스 이다.
* Key-Value Set 기반 데이터베이스

**![image-20210221211938033](http://www.jimbae.com:59005/image/155)**



### 쿠버네티스 - ETCD 데이터베이스 키 구조

![image-20210221220534755](http://www.jimbae.com:59005/image/156)



### 직접 ETCD 사용해보기

> 깃허브 주소 : https://github.com/etcd-io/etcd/releases

```bash
wget https://github.com/etcd-io/etcd/releases/download/v3.4.14/etcd-v3.4.14-linux-amd64.tar.gz #다운로드
tar -xf etcd-v3.4.14-linux-amd64.tar.gz #압축풀기
cd ./etcd-v3.4.14-linux-amd64 #폴더안에 etcdctl 명령어 존재
sudo ETCDCTL_API=3 ./etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get / --prefix --keys-only #ETCD 내장 내용 확인
```



**데이터 입출력 예제**

```bash
sudo ETCDCTL_API=3 ./etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key put key1 value1 #key1 value1 입력
sudo ETCDCTL_API=3 ./etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get key1  #key1 cnffur
```

---



## POD 소개

---

* 쿠버네티스에서의 기본 빌딩
* 팟은 하나의 노드에서 실행된다.



**장점**

* 밀접하게 연관된 프로세스를 함께 실행하고, 하나의 환경에서 동작하는 것처럼 보인다.
* 다소 격리된 상태로 유지 가능
  * 포드의 모든 컨테이너는 동일한 네트워크 및 UTS 네임스페이스에서 실행
  * 같은 호스트명 및 네트워크 인터페이스를 공유하므로 포트 충돌 가능성 있음



#### 포드 네트워크

* 쿠버네티스 클러스터의 모든 포드는 공유된 단일 플랫, 네트워크 주소 공간에 위치한다. [이미지 참조]
* 포드 사이에 NAT 게이트웨이가 존재하지는 않는다 [서로 통신은 되지만 NAT 게이트웨이 같지는 않다는 의미인듯 하다.]

![image-20210222204452781](http://www.jimbae.com:59005/image/162)



#### 컨테이너를 포드 전체에 적절하게 구성하는 방법

* 다수의 포드로 멀티티어로 어플리케이션 분할 필요함
* 각각 스케일링이 가능한 포드로 분할한다.

![image-20210222204803817](http://www.jimbae.com:59005/image/163)



#### 포드 정의

* 포드 정의 구성요소
  * apiVersion: 쿠버네티스 api의 버전
  * kind: 어떤 리소스 유형인지 결정[포드, 레플리카컨트롤러, 서비스 등]
  * 메타데이터: 포드와 관련된 이름, 네임스페이스 , 라벨, 그 밖의 정보 존재
  * 스펙: 컨테이너, 볼륨 등의 정보
  * 상태: 포드의 상태, 각 컨테이너의 설명 및 상태, 포드 내부의 IP 및 그 밖의 기본 정보 등





## 포드 디스크럽터 작성해보기

> https://kubernetes.io/docs/concepts/workloads/pods/



```bash
cd ~ #홈 디렉토리로 이동
mkdir yaml #yaml 설정 파일 저장할 폴더 생성
cd yaml
vim go-http-pod.yaml #yaml 생성
```



**go-http-pod.yaml**

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: http-go
spec:
    containers: #컨테이너 정보
    - name: http-go
      image: jimbae/http_go #docker hub 의 리포지토리 명
      ports:
      - containerPort: 8080
```

**포드 생성/확인**

```bash
kubectl create -f go-http-pod.yaml
kubectl get pod http-go -o yaml #yaml 형식으로 정보 상세보기 혹은 kubectl describe pod http-go 로도 확인 가능
```



**포드 삭제**

```bash
kubectl delete -f go-http-pod.yaml #또는
kubectl delete pod http-go
```



## 포드 실습 해보기

#### 연습문제

* 모든 리소스 삭제
* YAML을 사용하여 도커이미지 jenkins로 Jenkins-manual 포드를 생성하기
* Jenkins 포드에서 curl 명령어로 로컬호스트:8080 접속하기
* Jenkins 포트를 8888 로 포트 포워딩 하기
* 현재 Jenkins-manual의 설정을 yaml로 출력하기



**풀이**

```bash
kubectl delete pods --all #모드 포드 제거
vim jenkins-manual-pod.yaml #jenkins manual 포드 생성용 yaml 생성
kubectl create -f jenkins-manual-pod.yaml #jenkins manual pod 생성
kubectl exec jenkins-manual -- curl -s localhost:8080 #curl 로 테스트
kubectl port-forward jenkins-manual 8888:8080 # 포트포워딩
kubectl get pod jenkins-manual -o yaml #yaml 형태로 출력
kubectl logs jenkins-manual #logs 출력
```



**jenkins-manual-pod.yaml**

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: jenkins-manual
spec:
    containers: #컨테이너 정보
    - name: jenkins-manual
      image: jenkins:2.60.3
      ports:
      - containerPort: 8080
```



## POD의 라이브네스, 레드네스, 스타트업 프로브

* **Liveness Probe**
  * 컨테이너가 살았는지 판단하고, 아니면 다시시작하는 기능
  * 컨테이너의 상태를 스스로 판단하여 교착 상태에 빠진 컨테이너를 재시작
  * 버그가 생겨도 높은 가용성을 보임
* **Readiness Probe**
  * 포드가 준비된 상태에 있는지 확인하고 정상 서비스를 시작하는 기능
  * 포드가 적절하게 준비되지 않은 경우 로드밸런싱을 하지 않음
* **Startup Probe**
  * 애플리케이션의 시작 시기 확인하여 가용성을 높이는 기능
  * Liveness와 Readiness의 기능을 비활성화 하여 컨테이너 시작을 도움



#### 내용 실습

* https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ **[쿠버네티스 도큐맨테이션 참고]**

* **Liveness 커맨드**

  * 리눅스 환경에서 커맨드 실행 성공시 0
  * 리눅스 환경에서 커맨드 실행 실패시 기타값 -> 포드 재시작

* **Liveness 웹 설정 - http 요청 확인**

  * 서버 응답 코드가 200 이상 400 미만시 성공
  * 응답코드가 200이상 400미만이 아닐시 실패 -> 포드 재시작
    

  ##### LIVENESS 예제

  ```bash
  vim exec-liveness.yaml #Liveness 테스트용 yaml 생성
  kubectl create -f exec-liveness.yaml #실행하기
  kubectl describe pod liveness-exec #상태확인
  ```

  

  **exec-liveness.yaml [도큐맨테이션 중간에 확인가능]**

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    labels:
      test: liveness
    name: liveness-exec
  spec:
    containers:
    - name: liveness
      image: k8s.gcr.io/busybox
      args:
      - /bin/sh
      - -c
      - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
      livenessProbe:
        exec:
          command:
          - cat
          - /tmp/healthy
        initialDelaySeconds: 5
        periodSeconds: 5
  ```


  **LivenessProbe 결과 /tmp/healthy 가 응답 없음으로 killing 이 동작한것을 확인 가능**

  ![image-20210225173724069](http://www.jimbae.com:59005/image/165)



​		**LivenessProbe 로 인해 재시작이 여러차례 되었음을 확인 가능**

​		![image-20210225174048795](http://www.jimbae.com:59005/image/168) 



​		**LivenessProbe 로 인해 재시작이 여러차례 진행되었음을 describe 로 확인 가능**	

​		![image-20210225174240395](http://www.jimbae.com:59005/image/170)



​		**Liveness http 예제 [500] 이 나올시 재시작**

* ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    labels:
      test: liveness
    name: liveness-http
  spec:
    containers:
    - name: liveness
      image: k8s.gcr.io/liveness
      args:
      - /server
      livenessProbe:
        httpGet:
          path: /healthz
          port: 8080
          httpHeaders:
          - name: Custom-Header
            value: Awesome
        initialDelaySeconds: 3
        periodSeconds: 3
  ```

  

  **10초 전에는 200 10초 이후에는 500이 나오는 통신**

  ```go
  http.HandleFunc("/healthz", func(w http.ResponseWriter, r *http.Request) {
      duration := time.Now().Sub(started)
      if duration.Seconds() > 10 {
          w.WriteHeader(500)
          w.Write([]byte(fmt.Sprintf("error: %v", duration.Seconds())))
      } else {
          w.WriteHeader(200)
          w.Write([]byte("ok"))
      }
  })
  ```


  ```bash
  kubectl apply -f https://k8s.io/examples/pods/probe/http-liveness.yaml #위의 예제 실행
  ```

  

  **10초가 지나 Status code 500 이 나오고 재시작 되는 모습**

  ![image-20210225181423646](http://www.jimbae.com:59005/image/171)



* **Readiness 는 로드밸런싱 상황에서 확인해야 하므로 추후에 예제 추가한다.**

---







## 레이블과 셀렉터

---

* **레이블이란**
  * 모든 리소스를 구성하는 매우 간단하면서도 강력한 쿠버네티스 기능
  * 리소스에 첨부하는 임의의 키/값 쌍
  * 레이블 셀렉터 이용시 각종 리소스를 **필터링 하여 선택**가능
  * **포드를 인식하기 위한 바코드** 라고 한다.


* 예시) 레이블 생성후 분류별로 구분가능
  * ![image-20210225183828171](http://www.jimbae.com:59005/image/172)
  * app 은 어플리케이션의 정보
    * ui : User Interface
    * as : AccountService
    * ....
  * rel 은 버전의 대한 정보임
    * stable : 안정화 버전
    * beta : 테스트 버전
    * canary : 개발 버전



* **포드 생성시 레이블 지정법**

  ```yaml
  ...
  metadata:
  	name: 이름
  	labels:
  		app: ui
  		rel: beta
  ...
  ```

  

* **기존 포드에 추가하는법**

  ```bash
  kubectl label pod http-go-v2 rel=foo #rel: foo 와 같음
  kubectl label pod http-go-v2 app=ui --overwrite #기존에 이미 app 이라는 레이블이 등록되 있을경우 덮어쓰기는 --overwrite 옵션을 줘야함
  ```



* **레이블 삭제법**

  ```bash
  kubectl label pod http-go-v2 rel- #'-' 사용시 삭제
  ```



* **레이블 확인법**

  ```bash
  kubectl get pod --show-labels
  ```



* **특정 레이블 키로 값 확인하는법**

  ```bash
  kubectl get pod -L app,rel ##pod label중 app, rel key가 존재한다면 해당값 노출 
  ```



* **특정 레이블 필터링 하여 검색**

  ```bash
  kubectl get pod --show-labels -l 'env' #env label 을 가진 포드만 검색
  kubectl get pod --show-labels -l '!env' #env label 을 가지지 않은 포드만 검색
  kubectl get pod --show-labels -l 'env!=test' #env label이 test 가 아닌 포드만 검색
  kubectl get pod --show-labels -l 'env!=test,rel=beta' #env label이 test 가 아니고 rel label 값이 beta 인 경우만 검색
  ```



* 레이블 배치 전략

  * 레이블 생성시 사용할수 있는 예제들을 제공해주는 곳이 많음
    * https://www.replex.io/blog/9-best-practices-and-examples-for-working-with-kubernetes-labels

  ![image-20210225191000635](http://www.jimbae.com:59005/image/173)



#### 실습 진행

> https://kubernetes.io/docs/concepts/workloads/pods/ 에서 pods 생성문 참조

```bash
vim http-go-pad-v2.yaml
vim http-go-pad-v3.yaml
kubectl create -f http-go-pad-v2.yaml
kubectl create -f http-go-pad-v3.yaml
```



**http-go-pod-v2.yaml**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: http-go
  labels:  #레이블 추가 실습 진행
    creation_method: manual #Pod 생성 방식에 대한 명시, 수동으로 Pod 를 등록해주었으므로 manual
    env: prod #환경은 제품레벨 표기
spec:
  containers:
  - name: http-go
    image: jimbae/http_go
    ports:
    - containerPort: 8080
      protocol: TCP  	
```



**http-go-pod-v3.yaml**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: http-go-v3
  labels:  
    creation_method: manual-v3
spec:
  containers:
  - name: http-go
    image: jimbae/http_go
    ports:
    - containerPort: 8080
      protocol: TCP  	
```



**레이블 확인**

```bash
kubectl get pod --show-labels # 생성한 포드에 레이블 확인
```

![image-20210227204447617](http://www.jimbae.com:59005/image/175)



**레이블 컬럼값 확인하기**

```bash
kubectl get pod -L env #env 값 확인
kubectl get pod -L creation_method # creation_method 값 확인
```

![image-20210227204654883](http://www.jimbae.com:59005/image/176)



**레이블 추가하기**

```bash
kubectl label pod http-go test=foo
```

![image-20210227204808952](http://www.jimbae.com:59005/image/177)



**레이블 값 변경하기**

```bash
kubectl label pod http-go test=foo2 --overwrite
```

![image-20210227205515101](http://www.jimbae.com:59005/image/178)



**레이블 삭제하기**

```bash
kubectl label pod http-go test- # 키 뒤에- 붙이면 제거
```

![image-20210227205626286](http://www.jimbae.com:59005/image/179)



**레이블 필터링**

```bash
kubectl get pod -l env # 소문자 l 옵션 사용시 env 키를 가진 포드만 나오게됨
kubectl get pod -l '!env' # !를 앞에 붙이면 없는 아이만 나옴
kubectl get pod -l 'env=prod,creation_method=manual' # 조건문도 사용 가능
```

![image-20210227205742584](http://www.jimbae.com:59005/image/180)



---



## 레플리케이션 컨트롤러와 레플리카셋

### 레플리케이션컨트롤러

* 포드가 항상 실행되도록 유지하는 쿠버네티스 리소스
* 노드가 클러스터에서 사라지는 경우 해당 포드를 감치하고 대체 포드 생성
* 구버전에서 사용되고 `레플리카셋`으로 변경되고 있다고 한다.

![image-20210228235001829](http://www.jimbae.com:59005/image/182)





#### 레플리케이션컨트롤러의 세가지 요소

* 레플리케이션 컨트롤러가 관리하는 포드 범위를 결정 하는 `레이블 셀럭터`
* 실행해야 하는 포드의 수를 결정하는 `복제본 수`
* 새로운 포드의 모양을 설명하는 `포드 템플릿`



#### 레플리케이션컨트롤러의 장점

* 포드가 없는 경우 새 포드를 항상 실행
* 노드에 장애 발생 시 다른 노드에 복제본 생성
* 수동, 자동 수평 스케일링



#### 레플리케이션컨트롤러의 주의사항

* 레플리케이션컨트롤러는 레이블을 통해 Pod 가 살아있는지 여부를 확인하므로,
* 강제로 레이블 변경시 포드를 갯수에 맞춰서 신규생성한다.



### 실습 진행

> https://kubernetes.io/ko/docs/concepts/workloads/controllers/replicationcontroller/

#### Replicas Controller 설치

**http-go-rc.yaml**

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: http-go
spec:
  replicas: 3
  selector:
    app: http-go # labels 의 app 과 일치해야함
  template:
    metadata:
      name: http-go
      labels:
        app: http-go # selector 의 app 과 일치해야함
    spec:
      containers:
      - name: http-go
        image: jimbae/http_go
        ports:
        - containerPort: 8080
```



```bash
kubectl create -f http-go-rc.yaml #Replicas Controller 생성
kubectl get rc #Replicas Controller 확인
kubectl get pod -o wide #NODE 컬럼에서 어느 노드에 배치됬는지 확인가능하다
kubectl delete rc http-go # Replicas Controller 삭제
```

**배치된 Node 확인**![image-20210303222134976](http://www.jimbae.com:59005/image/215)



#### 레플리카셋 수정

##### scale 명령어 사용

```bash
kubectl scale rc http-go --replicas=5 #http-go rc replicas 5개로 수정
```



##### edit 명령어 사용

```bash
kubectl edit rc http-go 
...
replicas: 5 #이부분 수정
...
```



##### apply 명령어 사용

```bash
cp http-go-rc.yaml ./http-go-rc2.yaml
vim ./http-go-rc2.yaml
...
replicas: 5 #이부분 수정
...
kubectl apply -f http-go-rc2.yaml #수정한 내용 반영
```



### 레플리카셋

* 레플리카셋은 차세대 레플리케이션컨트롤러로 레플리케이션 컨트롤러를 완전히 대체 가능
* 일반적으로 레플리카셋을 직접 생성하지 않고 상위 수준의 디플로이먼트 리소스를 만들때 자동으로 생성된다

![image-20210308224723398](C:\Users\jimfo\AppData\Roaming\Typora\typora-user-images\image-20210308224723398.png)

#### 레플리케이션컨트롤러 vs 레플리카셋

* 레플리카셋이 더 풍부한 표현식 포드 셀렉터 사용 가능
  * 레플리케이션컨트롤러: 특정레이블을 포함하는 포드가 일치하는지 확인 (레이블 변경시 신규 레플리카스 생성)
  * 리플리카셋: 특정 레이블이 없거나 해당 값과 관계없이 특정 레이블 키를 포함하는 포드를 매치하는지 확인



#### 실습

* nginx를 3개 생성하는 rs-nginx 레플리카셋을 생성하라
* rs-nginx 포드의 개수를 10개로 스케일링 하라



> https://kubernetes.io/ko/docs/concepts/workloads/controllers/replicaset/ 공식문서 참조



**rs-nginx.yaml**

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-nginx
  #메타데이터의 label 은 필요 없다고 한다.
  #labels:
  #  app: rs-nginx
  #  tier: rs-nginx
spec:
  # 케이스에 따라 레플리카를 수정한다.
  replicas: 3
  selector:
    matchLabels:
      app: rs-nginx #앱의 이름으로 매칭한다
      #tier: rs-nginx
  template:
    metadata:
      labels:
        app: rs-nginx #앱의 이름으로 매칭한다
        #tier: rs-nginx
    spec:
      containers:
      - name: rs-nginx
        image: nginx:latest
        ports:
        - containerPort: 80 #80포트로 설정
```



**레플리카셋 생성**

```bash
kubectl apply -f rs-nginx.yaml #레플리카셋 생성
kubectl get rs #레플리카셋 확인
kubectl get pod #3개 확인
```



**10개로 스케일링 진행**

```bash
vim rs-nginx.yaml #레플리카셋 스케일링은 .sepc .replicas 필드를 수정하면 된다고 되있다.
kubectl apply -f rs-nginx.yaml #수정사항 적용
kubectl get rs
kubectl get pod #포드가 늘어난것 확인가능
```



---



## 디플로이 먼트

* 애플리케이션을 다운 타입 없이 업데이트 가능하도록 도와주는 리소스!
* 레플리카셋과 레플리케이션컨트롤러 상위에 배포되는 리소스

![image-20210305162513659](http://www.jimbae.com:59005/image/218)

* 모든 포드를 업데이트 하는 방법 [CD 의 개념인듯]
  * recreate: 잠깐의 다운 타임 발생 (새로운 포드를 실행시키고 작업이 완료되면 오래된 포드를 삭제)
  * 롤링 업데이트: 포드 여러개중 낮은 버전을 내리고 높은 버전으로 올리는 행위



#### 디플로이먼트 작성 요령

* Pod 의 metadata 부분과 spec을 그대로 옮겨서
* 배포할 포드를 설정



#### 실습

* jenkins 디플로이먼트 deploy-jenkins를 생성하라.
* jenkins 디플로이먼트로 배포되는 앱을 app: jenkins-test로 레이블링하라
* 디플로이먼트로 배포된 포드를 하나 삭제하고 이후 생성되는 포드를 관찰하라.
* 새로 생성된 포드의 레이블을 바꾸어 Deployment의 관리 영역에서 벗어나게 하라.
* Scale 명령을 사용해 레플리카 수를 5개로 정의한다.
* edit 기능을 사용하여 10로 스케일링 하라



> https://kubernetes.io/ko/docs/concepts/workloads/controllers/deployment/ 참조



**deploy-jenkins.yaml 생성**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-jenkins
  labels:
    app: jenkins-test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: jenkins-test
  template:
    metadata:
      labels:
        app: jenkins-test
    spec:
      containers: #컨테이너 정보
      - name: deploy-jenkins #강사는 jenkins 로 지정
        image: jenkins:2.60.3
        ports:
        - containerPort: 8080

```



**deploy-jenkins 디플로이먼트 생성**

```bash
kubectl apply -f ./deploy-jenkins.yaml #생성 강사는 create 사용
kubectl get all #모든거 확인 deployment 풀네임도 확인가능하다
kubectl get deployments #디플로이먼트 확인
```



**디플로이 된 포드 삭제후 관찰**

```bash
kubectl get pod #포드 확인
kubectl delete pod deploy-jenkins-78b4b6f59-gs5l9 #포드 삭제
kubectl get pod -w --show-labels -o wide #포드 상태 확인
```

![image-20210309205052744](http://www.jimbae.com:59005/image/223)



**새로 생성된 포드의 레이블을 바꾸어 Deploment의 관리 영역에서 벗어나게 하기**

```bash
kubectl label pod deploy-jenkins-78b4b6f59-zp964 app=jenkins-test-overwrite --overwrite 
```



**Scale 명령을 사용해 레플리카 수를 5개로 정의, edit 을 이용해 10개로 scale**

```bash
kubectl scale deployment.v1.apps/deploy-jenkins --replicas=5
#vim deploy-jenkins.yaml #replicas: 10 으로 변경
#kubectl apply -f ./deploy-jenkins.yaml
kubectl edit deploy deploy-jenkins #에서 replicas: 10 으로 변경
```



---



## 롤링 업데이트와 롤백

### 기존 방식

* 기존 모든 포드를 삭제 후 새로운 포드 생성
  * 잠깐의 다운 타임 발생

![image-20210306221731063](http://www.jimbae.com:59005/image/219)



### 롤링 업데이트 방식

* 새로운 포드를 실행시키고 작업이 완료되면 오래된 포드를 삭제
  * 새 버전을 실행하는 동안 구 버전 포드와 연결
  * 서비스의 레이블 셀렉터를 수정하여 간단하게 수정 가능

![image-20210306222211294](http://www.jimbae.com:59005/image/220)

* 주의사항:
  * v1과 v2 는 호환이 가능해야함.



## 롤링 업데이트 방법

* 디플로이먼트 업데이트 전략(StrategyType)

  * Rolling Update(기본값)
    * 오래된 포드를 하나씩 제거하는 동시에 새로운 포드 추가
    * 요청을 처리할 수 있는 양은 그대로 유지
    * 반드시 이전 버전과 새 버전을 동시에 처리 가능하도록 설계한 경우에만 사용(호환)
  * Recreate
    * 새 포드를 만들기 전에 이전 포드를 모두 삭제
    * 여러 버전을 도시에 실행 불가능
    * 잠깐의 **다운 타임** 존재

  ```yaml
  ...
  spec:
  	strategy:
  		type: RollingUpdate #default
  ...
  ```



## 롤백 실행하는 방법

* 롤백 실행시 이전 업데이트 상태로 돌아감

* 롤백을 하여도 히스토리의 리비전 상태는 이전 상태로 돌아가지 않음

  ```bash
  kubectl set image deployment htt-go http-go-gasbugs/http-go:v3 #업데이트
  kubectl rollout undo deployment http-go #이전 버전으로 롤백
  kubectl rollout undo deployment http-go --to-revision=1 # v1 로 롤백
  ```

  

### 업데이트를 실패하는 경우 [기본 600 초]

* 부족할 할당량(Insufficient quota)
  * CPU나 메모리 부족
* 레디네스 프로브 실패(Readiness probe failures)
  * Pod 가 준비되지 않을때
* 이미지 가져오기 오류(Image pull errors)
  * 이미지가 없는경우
* 권한 부족(Insufficient permission)
* 제한 범위(Limit ranges)
  * 자원의 양을 벗어나는 경우
* 응용 프로그램 런타임 구성 오류(Application Runtime misconfiguration)
  * 어플리케이션 실행중 에러



### 실습

**yaml 생성**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: http-go
  labels:
    app: http-go
spec:
  replicas: 3
  selector:
    matchLabels:
      app: http-go
  template:
    metadata:
      labels:
        app: http-go
    spec:
      containers: #컨테이너 정보
      - name: http-go
        image: jimbae/http_go:v1
        ports:
        - containerPort: 80
```



**실행중인 디플로이먼트 정보 yaml 형식으로 확인**

```bash
kubectl get deploy http-go -o yaml
```

![image-20210311230852867](http://www.jimbae.com:59005/image/231)

* `progressDeadlineSeconds` : 업데이트 실패시 undo 실행 시간
* `revisionHistoryLimit` : 업데이트 실패시 revision 할수 있도록 상태 저장하는 개수
* `rollingupdate` : 롤링 업데이트시 pod 개수 



**rivision 을 생성하면서 deploy 진행**

```bash
kubectl create -f http-go-deploy-v1.yaml --record=true
kubectl rollout status deploy http-go #rollout 상태 확인
kubectl rollout history deploy http-go #revision 확인
```



**롤링 업데이트 관찰을 위한 minReadySeconds 옵션 변경**

```bash
kubectl patch deploy http-go -p '{"spec":{"minReadySeconds": 10}}' #업데이트 속도를 늦추기 위해 옵션조정
```



```bash
kubectl expose deploy http-go #로드 밸런싱을 위한 명령어
```



**변경 확인을 위한 busybox 실행**

```bash
kubectl run -it --rm --image busybox -- bash
#while true; wget -O- -q [clusterIP:Port]; sleep 1; done #쉘 스크립트 생성
```



**업데이트 진행**

```bash
kubectl set image deploy http-go http-go=gasbugs/http-go:v2 --record=true
kubectl rollout history deploy http-go #revision 확인시 변경 확인 가능
kubectl get all # 확인시 기존의 레플리카셋은 살아잇지만 포드가 0개로 백업확인 가능
```



**업데이트 진행2**

```bash
kubectl edit deploy http-go --record=true
...
image: jimbae/http_go:v2 #수정
...
```



**undo 를 통한 롤백**

```bash
kubectl rollout undo deploy http-go #이전 버전으로 rollback
```



**특정 rivision 으로 롤백**

```bash
kubectl rollout undo deploy http-go --to-revision=1 #1번 revision 으로 rollback
```



---



## 네임스페이스

* 리소스를 각각의 분리된 영역으로 나누기 좋은 방법
* 여러 네임스페이스를 사용하면 복잡한 쿠버네티스 시스템을 더 작은 그룹으로 분할
* 리소스 이름은 네임스페이스 내에서만 고유 명칭 사용 > 네임스페이스 별 중복 이름 사용 가능



### 네임스페이스 확인

```bash
kubectl get ns
kubectl get [조회하고자 하는 이름] -n [ns 이름] #특정 namespace 에서 조회 하고자하는 항목 선택 -n 옵션이 없으면 default ns 에서 조회
```

![image-20210313215544723](http://www.jimbae.com:59005/image/233)



#### 전체 네임스페이스에 대한 조회

```bash
kubectl get pod --all-namespaces #모든 네임스페이스의 pod 확인
```





### 네임스페이스 만들기

#### yaml 로 만들기

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: test-ns
```



#### kubectl 명령어 활용

```bash
kubectl create namespace "test-namespace"
```



### 실습

```bash
kubectl create ns office #office ns 생성
or
kubectl create ns office2 --dry-run=client -o yaml > ./office-ns.yaml #yaml 생성
```



**ns 에 nginx 시작하기**

```bash
kubectl create deploy nginx --image nginx --port 80 -n office #office namespace 에 nginx pod 생성
kubectl get pod -n office # 이런식으로 해야 나옴
```



**기본 네임스페이스 office로 변경**

```bash
vim ~/.kube/config
...
contexts:
- context:
	cluster: kubernetes
	user: kubernetest-admin
	namespace: office #기본은 default 이므로 해당 라인 추가
...
```



**ns 삭제**

```bash
kubectl delete ns office
```



### 연습문제

* 현재 시스템에는 몇 개의 Namespace 가 존재하는가?

  ```bash
  kubectl get ns | wc -l #(컬럼)-1 해서 4 
  ```

* kube-system에는 몇 개의 포드가 존재하는가?

  ```bash
  kubectl get pod -n kube-system | wc -l#(컬럼)-1 해서 13
  ```

* ns-jenkins 네임스페이스를 생성하고 jenkins 포드를 배치하라

  * ns-jenkins ns 생성

    ```bash
    kubectl create namespace ns-jenkins
    ```

  * jenkins pod 생성

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
        name: jenkins
        namespace: ns-jenkins
    spec:
        containers: #컨테이너 정보
        - name: jenkins
          image: jenkins:2.60.3
          ports:
          - containerPort: 8080
    ```

    * 강의 제작자 풀이

      ```bash
      kubectl create namespace ns-jenkins --dry-run=client -o yaml > ns-jenkins.yaml #생성
      ```

      **ns-jenkins.yaml**

      ```yaml
      apiVersion: v1
      kind: Namespace
      metadata:
        name: ns-jenkins
      
      --- #문서를 끊어준다
      
      apiVersion: v1
      kind: Pod
      metadata:
          name: jenkins
          namespace: ns-jenkins
      spec:
          containers: #컨테이너 정보
          - name: jenkins
            image: jenkins:2.60.3
            ports:
            - containerPort: 8080
      ```

      

    ```bash
    kubectl apply -f ns-jenkins.yaml 
    ```

* coredns는 어느 네임스페이스에 속해있는가?

  ```bash
  kubectl get all --all-namespaces | grep coredns#확인결과 kube-system 에 위치한 디플로이먼트
  ```

  



## 서비스와 ClusterIP

> 포드의 문제 때문에 서비스가 필요하다.
> 포드는 쿠버네트스 개별의 클러스터에 네트워크를 가지고 있어서 외부와 통신이 안된다.



#### 포드의 문제점

* 포드는 일시적으로 생성한 컨테이너의 집합
* 때문에 포드가 지속적으로 생겨났을 때 서비스를 하기에 적합하지 않다.
* IP 주소의 지속적인 변동, 로드밸런싱을 관리해줄 다른 객체 필요.
* 이 문제를 해결 하기 위해 `서비스` 가 필요하다

![image-20210315225955775](http://www.jimbae.com:59005/image/234)

> 노드의 오류로 포드 위치 변경시 IP 도 변경되는 문제가 있다



#### 서비스의 요구사항

* 외부 클라이언트가 몇 개이든지 프론트엔드 포드로 연결되어야 한다.
* 포드의 IP가 변경될 때마다 재설정 하지 않도록 해야한다.

![image-20210315230218300](http://www.jimbae.com:59005/image/235)

> 클러스트의 변경이나 확장일때, 생성된 포드로 연결해줄 서비스가 필요하다.



#### 서비스 생성 방법 / 다중 포트 사용 방법

* kubectl 의 expose가 가장 쉬운 방법
* YAML을 통해 버전 관리 가능
* ports 부분에 연속적으로 입력하면 다중 포트 사용 가능

```yaml
apiVersion: v1
kind: Service
metadata:
	name: http-go-svc
spec:
	ports: #s가 붙는 복수형은 하위 옵션에 - 를 사용하여 배열 생성
	- name: http
	  port: 80 #외부 포트
	  targetPort: 8080 #포드 포트
	- name: https
	  port: 443 #외부 포트
	  targetPort: 8443 #포드 포트	  
	selector:
	  app: http-go
```



#### 포드 간의 통신을 위한 ClusterIP

* 내부에서의 통신을 위한 (백앤드 서버 & 프론트앤드 서버...) 다수의 포드를 하나의 서비스로 묶어서 관리할수 있도록 도와주는것이 ClusterIP 이다

![image-20210315231701805](http://www.jimbae.com:59005/image/237)



#### 서비스의 세션 고정하기

* 서비스가 다수의 포드로 구성하면 웹서비스의 세션이 유지되지 않음
* 이를 위해 처음 들어왔던 클라이언트 IP를 그대로 유지해주는 방법이 필요
* sessionAffinity: ClientIP라는 옵션을 주면 해결 완료. 처음 붙은 포드의 IP로 계속 사용하도록 해주는 기능

![image-20210315232200536](http://www.jimbae.com:59005/image/238)



#### 서비스 상세조회

```bash
kubectl describe svc [서비스명]
```



#### 외부 IP 연결 설정

* 쿠버네티스를 처음 도입하는 경우나 외부에 서비스가 존재하는 경우에도 Endpoint를 이용해 로드밸런싱이 가능하다

```yaml
apiVersion: v1
kind: Endpoints
metadata:
	name: external-service
subsets:
	- addresses:
	  - ip: 11.11.11.11
	  - ip: 22.22.22.22
      ports:
      - port: 80
```

![image-20210315232956672](http://www.jimbae.com:59005/image/239)



#### 실습

* http-go.yaml 생성

  ```bash
  kubectl create deploy --image=jimbae/http_go htto-go --port=8080 --dry-run -o yaml > http-go.yaml
  ```

* yaml 수정

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
  	name: http-go-svc
  spec:
  	selector:
  		app: http-go
  	ports:
  		- protocol: TCP
  		  port: 80
  		  targetPort: 8080
  		
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
  	creationTimestamp: null
  	labels:
  		app: http-go
  	name: http-go
  spec:
  	replicas: 1
  	selector:
  		matchLabels:
  			app: http-go
  	template:
  		metadata:
  			creationTimestamp: null
  			labels:
  				app: http-go
          spec:
          	containers:
          	- image: jimbae/http_go
          	  name: http-go-ttz54
          	  ports:
          	  - containerPort: 8080
  ```

* yaml실행

  ```bash
  kubectl apply -f http-go.yaml
  ```

*  pod IP 와 svc 의 endPoint IP 가 같은것을 확인 가능

  ```bash
  kubectl get pod -o wide # 10.32.1.7
  kubectl describe svc
  ```

  ![image-20210316214809149](http://www.jimbae.com:59005/image/242)

* Scale 시 Endpoint 확인

  ```bash
  kubectl scale deploy http-go --replicas=5
  ```

  ![image-20210316215233695](http://www.jimbae.com:59005/image/243)

* sessionAffinity 변경을 통한 테스트

  ```bash
  kubectl edit svc http-go-svc
  ...
  	sessionAffinity: ClientIP
  ...
  ```

* sessionAffinity 테스트

  ![image-20210316215837214](http://www.jimbae.com:59005/image/245)

  > sessionAffinity Option 이 ClientIP 이므로 로드밸런싱이 되지 않고 있음. [처음 접속자는 계속 한 세션에 접근]



## 외부에 서비스하는 방법과 노드 포트

#### 서비스를 노출하는 3가지 방법

* NodePort: 노드의 자체 포트를 사용하여 포드로 리다이렉션
* LoadBalancer: 외부 게이트웨이를 사용해 노드 포트로 리다이렉션
* Ingress: 하나의 IP 주소를 통해 여러 서비스를 제공하는 특별한 메커니즘



#### 노드포트[NodePort] 생성하기

* 서비스 yaml 파일을 작성
* type에 NodePort를 지정
* 30000~32767 포트만 사용가능 [변경은 가능하다.] [외부의 공개되는 포트와는 별개다]

```yaml
apiVersion: v1
kind: Service
metadata:
	name: http-go-svc
spec:
	type: NodePort
	selector:
		app: http-go
	ports:
		- protocol: TCP
		  port: 80 #서비스의 포트
		  targetPort: 8080 # 포드의 포트
		  nodePort: 30001 # 최종적으로 서비스 되는 포트
```



* 노드포트를 활용한 로드밸런싱

  ![image-20210317223649827](http://www.jimbae.com:59005/image/246)



#### 실습하기

* yaml 작성 

  **http-go-np.yaml**

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
  	name: http-go-np
  spec:
  	type: NodePort
  	selector:
  		app: http-go
  	ports:
  		- protocol: TCP
  		  port: 80
  		  targetPort: 8080
  		  nodePort: 30001
  ```

  

![image-20210318223451680](http://www.jimbae.com:59005/image/247)

> Type : NodePort, Port: 30001 확인 가능
>
> 연결된 외부 아이피의 경우 Node IP 를 확인하면 된다 [kubectl get nodes -o wide]



* GCP 의 경우 방화벽 해제

  ```bash
  gcloud compute firewall-rules create http-go-svc-rule --allow=tcp:30001 #30001 번 포트 열기
  ```

  

* 포드 확인하기

  ```bash
  curl [nodeIP]:30001 
  ```

  

* 로드밸런스 yaml 작성하기

  **http-go-lb.yaml**

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
  	name: http-go-lb
  spec:
  	type: LoadBalancer
  	selector:
  		app: http-go
  	ports:
  		- protocol: TCP
  		  port: 80 #외부에 공개될 포트
  		  targetPort: 8080
  ```

* SVC 정상 생성및 외부 IP 할당정보 확인 가능 [GCP]

  ![image-20210318231454465](http://www.jimbae.com:59005/image/248)

* 해당 IP 사용시 외부에서 조회 가능

![image-20210318231744117](http://www.jimbae.com:59005/image/249)